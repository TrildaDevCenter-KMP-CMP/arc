"use strict";(self.webpackChunkarc=self.webpackChunkarc||[]).push([[846],{1032:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>i,contentTitle:()=>o,default:()=>d,frontMatter:()=>r,metadata:()=>s,toc:()=>c});var a=n(4848),l=n(8453);const r={title:"Ollama Client"},o=void 0,s={id:"clients/ollama",title:"Ollama Client",description:"The Ollama Client connects to the Completion API of a running Ollama server.",source:"@site/docs/02-clients/ollama.md",sourceDirName:"02-clients",slug:"/clients/ollama",permalink:"/arc/docs/clients/ollama",draft:!1,unlisted:!1,tags:[],version:"current",frontMatter:{title:"Ollama Client"},sidebar:"tutorialSidebar",previous:{title:"Google Gemini",permalink:"/arc/docs/clients/gemini"},next:{title:"Readers",permalink:"/arc/docs/readers/"}},i={},c=[];function m(e){const t={a:"a",code:"code",p:"p",pre:"pre",...(0,l.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(t.p,{children:"The Ollama Client connects to the Completion API of a running Ollama server."}),"\n",(0,a.jsx)(t.p,{children:"Ollama is a great way to run LLM models locally. Even without expensive hardware, smaller models,\nsuch as Gemma 7B, can be a great way to start experimenting with Arc Agents."}),"\n",(0,a.jsxs)(t.p,{children:["See ",(0,a.jsx)(t.a,{href:"https://ollama.com/",children:"https://ollama.com/"})," for more details."]}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-kotlin",children:'val client = OllamaClient(OllamaClientConfig("modelName", "url"), eventPublisher)\n'})})]})}function d(e={}){const{wrapper:t}={...(0,l.R)(),...e.components};return t?(0,a.jsx)(t,{...e,children:(0,a.jsx)(m,{...e})}):m(e)}},8453:(e,t,n)=>{n.d(t,{R:()=>o,x:()=>s});var a=n(6540);const l={},r=a.createContext(l);function o(e){const t=a.useContext(r);return a.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function s(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(l):e.components||l:o(e.components),a.createElement(r.Provider,{value:t},e.children)}}}]);