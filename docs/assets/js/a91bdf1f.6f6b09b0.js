"use strict";(self.webpackChunkarc=self.webpackChunkarc||[]).push([[67],{7739:a=>{a.exports=JSON.parse('{"blogPosts":[{"id":"/Llama3","metadata":{"permalink":"/arc/blog/Llama3","source":"@site/blog/Llama3.md","title":"Llama3 is out!","description":"Meta have released a new version of their large language model, Llama3, https://ai.meta.com/blog/meta-llama-3/.","date":"2024-04-25T16:22:42.000Z","tags":[],"readingTime":0.35,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Llama3 is out!"},"unlisted":false},"content":"Meta have released a new version of their large language model, Llama3, https://ai.meta.com/blog/meta-llama-3/.\\n\\nThanks to ollama, the model can easily be tested locally, see https://ollama.com/library/llama3\\n\\nSimply pull the model, run `ollama` and start using it with Arc!\\n\\n```bash\\nollama pull llama3:8b\\nollama serve\\n```\\n\\nNow llama3:8b is ready to use with Arc and the ollama client.\\nWe recommend using the 8B variant locally, as it \\"only\\" requires 8GB of RAM."}]}')}}]);